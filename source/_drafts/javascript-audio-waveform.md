---
title: 컴퓨터는 어떻게 소리를 들을까?
toc: true
widgets:
  - type: toc
    position: right
  - type: category
    position: right
sidebar:
  right:
    sticky: true
tags:
  - Audio
  - Sound Engineering
categories:
  - JavaScript
thumbnail: /2019/07/09/javascript-audio-waveform/thumbnail.jpg
---

이번 포스팅에서는 필자의 예전 직업이었던 `사운드 엔지니어`의 추억을 살려서 한번 오디오에 대한 이론을 설명해볼까 한다. 하지만 설명만 하면 재미가 없기에 오디로 이론을 기반으로 자바스크립트의 `Audio Context API`를 사용하여 간단한 오디오 파형을 그려보려고 한다.

<!-- more -->

먼저 기본적인 오디오에 관한 지식이 있어야 해당 작업을 좀 수월하게 할 수 있으므로 이론을 최대한 지루하지 않게 설명해보겠다. 이론이 코딩보다 재미없는 건 맞지만 오디오 파형을 그리려면 기본적으로 알고 있어야 하는 것들이다.

## 소리란 무엇일까?
제일 먼저 소리가 무엇인지부터 알아보자! 소리를 딱 한마디로 표현하자면 바로 `진동`이다. 기본적으로 우리가 소리를 듣는다는 것은 이런 순서로 일어난다.

***
1. 어떤 물체가 부르르 진동을 한다. 이때 진동 주파수는 뭐 대충 `440hz`라고 치자.
2. 그 물체 주변에 있는 매질이 `440hz`의 진동을 전달한다.(일반적인 상황에서는 주로 공기)
3. 매질이 진동을 전달하면 우리의 고막도 `440hz`로 진동한다.
4. 그 진동을 청신경에 전달한다.
5. 뇌가 신호를 받아서 해석한다. `440hz` 접수완료!
***

이때 `1번` 순서에서 물체가 1초에 몇번이나 떨렸는지를 표현하기 위해 우리는 `헤르츠(herz, hz)`단위를 사용한다. `10hz`는 1초에 10번 진동을 했다는 의미이고 `1khz`는 1초에 1000번 진동을 했다는 것이다. 참고로 예시의 `440hz`는 도레미파솔라시도할 때 `라`음이다. 우리가 음악을 들을 때는 현악기면 `현의 진동`, 관악기면 `입술의 진동이 증폭된 것`, 노래라면 `성대의 진동`을 듣게 되는 것이다. 아무리 감성터지는 음악도 공돌이 손에 걸리면 이렇게 분해될 수 있다.

이때 이 진동은 자연계에서 발생한 것이기 때문에 `아날로그(Analog)`의 형태로 나타난다. 사실 자연계에서 발생하는 대부분의 거시적인 신호는 아날로그 형태를 가지는데, 예를 들면 빛의 밝기가 변한다거나 바람의 세기가 변한다거나 소리의 크기가 변하는 등의 신호를 말한다.

### 아날로그 신호
`아날로그`는 신호나 자료를 `연속적인` 물리량으로 나타낸 것이다. 연속적이란 것은 무엇일까? 이게 `연속적인 물리량`이라고 하면 뭔가 전문적이고 어려워보이는데 풀어보면 사실 별 거 없다. 여기서 우리가 집중해야할 단어는 물리량 어쩌고가 아닌 `연속적`이다.

<center>
  {% asset_img analog.png 400 %}
  <small>가로 축은 시간, 세로 축은 전압이다. 아날로그 신호는 아무리 쪼개도 끝이 없는 연속성을 가진다</small>
  <br>
</center>

여러분이 여름철 뙤약볕에 앉아 있다고 생각해보자. 이때 바람이 솔솔 불다가 점점 세게 불어서 여러분은 시원함을 느낀다. 이때 포인트는 여러분이 느끼는 시원함은 `점점 증가한다`는 것이다. 계속 덥다고 느끼다가 어느 순간 갑자기 `어? 갑자기 시원해졌어!`라고 느끼지는 않는다.

또 하나 `아날로그`의 대표적인 예는 바로 시간이다. 자 지금 딱 이 시점부터 `1초 후`의 시간까지를 생각해보자.
이 시점부터 `1초 후`의 사이에는 `0.5초 후`도 있을 것이고 `0.25초 후`도 있을 것이며 `0.125초 후`도 있을 것이다. 우리가 지금부터 `1초 후` 사이의 시간을 아무리 쪼개본다 한들 이 쪼개지는 단위에 한계는 없다. 만약 $10^{-10000}$초까지 쪼갠다 해도 우리는 그 시간을 더 쪼갤 수 있다.

이제 연속적이라는 말이 조금 이해가 되었길 바란다. 그리고 사실 우리가 `1990년대 ~ 2000년대 초반`까지 많이 사용하던 `전화기`나 `TV`도 전부 아날로그 방식이었다. 지금처럼 디지털로 처리하게 된 건 생각보다 얼마 되지 않았다...라고 하기에는 20~30년 전이라서 그냥 필자가 늙은 걸로 하겠다. 혹시 주민번호 뒷자리가 `3`이나 `4`로 시작하시는 분들도 이걸 한번쯤은 사용해봤을거라 믿는다.

<center>
  {% asset_img analog-phone.jpg 500 %}
  <small>갬성 레트로의 상징. 이게 또 다이얼 돌렸다가 놨을 때 촤라라라락 딸깍! 하는 손맛이 있다.</small>
  <br>
</center>

저 당시 우리가 사용하는 전화기는 우리가 말하는 목소리의 진동을 그대로 아날로그인 전기 신호로 변경해서 상대방의 전화기까지 보내는 것이다. 실전화와 원리는 같지만 실전화가 우리의 목소리를 그대로 실을 통해서 전달했다면 전화기는 더 멀리 소리를 보내기 위해 목소리를 전기 신호로 바꾼 후 보내는 것이다.

이렇게 아날로그는 우리에게 굉장히 친숙한 신호다.

## 컴퓨터가 소리를 듣는 방법
근데 여러분도 알다시피 컴퓨터는 `0`하고 `1`밖에 이해하지 못하는 바보다. 이 방식을 우리는 `디지털(Digital)`이라고 부른다. 그렇다면 우리가 자연에서 발생한 아날로그 형태인 소리를 컴퓨터가 듣게 해주려면 어떻게 해야할까?
간단하다. 아날로그를 디지털로 바꿔주면 된다. 그럼 지금부터 아날로그를 디지털로 변경하는 `Analog to Digital`에 대해서 알아보자.

아날로그를 디지털로 변경하기 위해서는 몇가지 순서를 거쳐야 한다. 이 순서에서 어떤 값들을 사용하냐에 따라서 디지털로 변경된 소리의 해상도, 즉 음질이 결정된다.
이때 나오는 단어가 바로 `샘플 레이트(Sample Rate)`와 `비트 레이트(Bit Rate)`이다.

<center>
  {% asset_img encoder.png 500 %}
  <small>인코더 프로그램 쓸 때 뭘 만져야 할지 모르게 만드는 어려운 단어들</small>
  <br>
</center>

단어는 어려워 보이지만 사실 간단하다. 결국 소리는 가로 축은 `시간(Time)`, 세로 축은 `진폭(Amplitude)`이라는 캔버스에 그려진 2차원의 진동 주파수 데이터이다. 이때 `샘플 레이트`는 가로 축의 해상도, `비트 레이트`는 세로 축의 해상도를 의미하는 것이다. 우선 `샘플 레이트`부터 한번 파헤쳐보자.

### 샘플링(Sampling)
`샘플링`은 아날로그 신호를 디지털 신호로 바꾸기 위한 첫번째 과정이다. 위에서 설명했듯이 아날로그 신호인 소리는 연속적인 신호이기 때문에 컴퓨터가 이 신호를 그대로 이해할 수가 없다. 우리가 마이크를 사용하여 소리를 녹음할 때, 이 아날로그 신호는 결국 전화기와 같은 원리로 전기 신호로 변환되어 컴퓨터에게 주어진다. 하지만 컴퓨터는 당연히 이 연속적인 전기 신호를 이해할 수 없다.

그래서 컴퓨터는 연속적인 전기 신호를 측정하기 위해 특정 타이밍을 정해서 `이 타이밍마다 내가 전압을 측정할게!`라는 꼼수를 사용한다. 컴퓨터가 꼼수를 쓰는 이 과정을 그림으로 나타내보면 이런 느낌이다.

<center>
  {% asset_img sampling.png 500 %}
  <br>
</center>

위 그림에 나타난 빨간 점이 컴퓨터가 전압을 측정한 타이밍이다. 컴퓨터는 특정 타이밍에 전기 신호를 측정하고 그 값을 저장한다. 빨간 점의 위치를 보면 저 신호는 `[10, 20, 30, 27, 19, 8...]` 뭐 이런 식으로 측정이 되었을 것이다.
이때 저 빨간 점을 더 세밀하게, 즉 컴퓨터가 샘플 측정을 하는 간격이 짧을 수록 우리는 원래 신호에 가까운 값을 측정할 수 있다.

<center>
  {% asset_img high-low-sampling.png 500 %}
  <small>신호 내부의 사각형이 컴퓨터가 이해한 신호의 모양이다.</small>
  <br>
</center>

이때 이 신호를 측정하는 간격을 `샘플 레이트(Sample Rate)`라고 하고 신호를 측정하는 과정 자체를 `샘플링(Sampling)`이라고 한다. 당연히 샘플 레이트가 높을 수록 소리의 해상도, 즉 음질이 더 좋을 수 밖에 없다. 특히 높은 주파수를 가진 소리, 즉 고음의 해상도가 확연하게 좋아진다.
보통 CD의 음질이 `44.1kHz`, TV나 라디오 방송이 `48kHz`의 샘플 레이트를 가지는데, 이는 약 1초에 `44,100`번, `48,000`번 샘플을 측정한다는 것이다. 근데 저 샘플 레이트는 어떤 기준으로 정하는 걸까?

### 샘플 레이트(Sample Rate)를 좀 더 자세히 알아보자
방금 설명했듯이 CD의 경우 샘플 레이트가 `44.1kHz`이기 때문에 컴퓨터는 아날로그 신호를 1초에 `44,100`번 측정한다.
하지만 인간이 들을 수 있는 영역인 가청주파수는 `20hz ~ 20kHz` 밖에 되지 않는다. 근데 왜 샘플 레이트는 `44.1kHz`, `48kHz`처럼 크게 잡는 것일까?

> 어차피 인간은 1초에 `20,000`번 진동하는 소리까지밖에 들을 수 없어서 `44,100`번 진동하는 소리를 녹음해도 어차피 들을 수 없는데?

이 질문에 대한 답은 소리의 진동 사이클이 어떻게 생겼는지를 보면 이해가 된다.

<center>
  {% asset_img sine-wave.png 500 %}
  <br>
</center>

기본적으로 오디오 주파수는 이렇게 하나의 사이클 단위로 나누어 지는데, 이때 위로 올라가는 `+` 부분이 공기가 압축되는 부분이고 아래로 내려가는 `-` 부분이 다시 공기가 팽창하는 부분이다.

위에서 계속 설명했던 대로 소리란 곧 진동이고, 우리가 느끼는 것은 그 진동으로 인한 공기의 `떨림`이므로 `압축 -> 팽창 -> 압축`까지 모두 들어야 `떨렸다!`라고 느낄 수 있다는 것이다.
압축이나 팽창 중에 하나만 주구장창 느낀다고 해서 `이게 진동이구나`라고 느낄 수는 없을 것이다.

그래서 오디오 신호의 한 사이클을 제대로 측정하려면 `+ 방향의 맨 위의 꼭지점 하나`와 `- 방향의 맨 밑의 꼭지점 하나`를 모두 측정해야하기 때문에 최소 `2번`은 측정을 해야하는 것이다. 그래서 인간이 들을 수 있는 가장 높은 소리인 `초당 22,000의 떨림`인 `22kHz`을 제대로 측정하려면 우리는 `22,000 * 2 = 44,000번 = 44kHz`의 그릇을 준비해야한다.

이게 바로 CD가 왜 `44.1kHz`의 샘플 레이트를 가지고 있는지에 대한 이유다. 이걸 `나이퀴스트 이론(Nyquist Theorem)`이라고 한다. 즉, `나이퀴스트 이론`을 한마디로 정리하자면

> 측정하고 싶은 오디오 주파수있지? 오디오 신호 제대로 다 살리고 싶으면 최소한 그 주파수보다 두배는 더 빠르게 측정해야된다.
> 그러니까 최소한 측정하고 싶은 오디오 주파수의 두배 사이즈의 샘플 레이트를 준비하렴.

인 것이다. 근데 여기서 또 의문이 생긴다. 저 이론에 따르면 인간의 가청주파수는 `22,000hz`니까 딱 `44,000`번만 측정하면 인간이 들을 수 있는 소리는 다 녹음할 수 있는데 왜 `44,100`번이나 `48,000`번까지 측정하는 걸까?

<center>
  {% asset_img dolphin.jpg 500 %}
  <small>안녕하세요 자연계 고음의 절대강자입니다. 엠씨더맥스 사계 정도는 밥먹으면서도 부를 수 있슴다.</small>
  <br>
</center>

사실 자연에는 우리가 듣지 못하는 훨씬 높은 소리들도 존재한다. 단지 우리가 `20kHz`까지밖에 못 들을 뿐이다. 뭐 박쥐나 돌고래 같은 친구들은 훨씬 고음역대의 소리를 내지 않는가?
근데 이 소리가 `44kHz`의 샘플 레이트를 준비한 그릇에 들어오면 어떻게 될까? 컴퓨터는 1초에 `20,000`번의 사이클을 도는 소리를 제대로 측정하려고 1초에 `44,000`번 전압을 측정하려고 했는데 만약 1초에 `30,000`번의 사이클을 도는 소리가 들어와버린다면?

<center>
  {% asset_img nyquist-error.png 500 %}
  <br>
</center>

> 정답. 점을 이상한데다가 찍는다.

그림을 보면 컴퓨터가 점을 찍는 간격, 즉 전압을 측정하는 간격보다 들어온 신호의 사이클이 더 짧다. 그래서 컴퓨터가 찍은 점을 보면 신호의 꼭지점이 아닌 어중간한 어딘가에 찍힌 것을 볼 수 있다. 이것이 바로 `나이퀴스트 이론`의 가지고 있는 함정이다. 그리고 저 어중간한데 찍힌 점들을 이어본 파란색 선을 보면 결국 낮은 주파수가 된 것을 알 수 있다. 그러면 어떻게 될까?

우리 귀에 아주 잘 들린다. 참 소름돋는 순간이다! 녹음할 때는 분명히 아무것도 안들렸는데 녹음한 걸 들어보니 이상한 소리가 녹음되어있으니 말이다. 그래서 이 현상을 `고스트 주파수(Ghost Frequency)`라고 부른다.

그래서 이걸 해결하기 위해 사용한 방법이 바로 `LPF(Low Pass Filter)`이다. 이 필터는 전기 쪽 공부하신 분들은 매우 익숙할텐데, 말 그대로 낮은(Low) 주파수만 통과(Pass)시키는 필터이다.
오디오 녹음을 할때 `LPF`를 사용해서 인간의 가청주파수보다 높은 소리는 다 잘라버리고 인간의 가청주파수 영역의 소리만 통과시키면 방금 얘기한 고스트 주파수가 생길 일도 없기 때문이다.

근데 또 아날로그 신호라는 게 그렇게 무 자르듯이 싹뚝! 잘리는 게 아니다.

<center>
  {% asset_img cut-off.png 500 %}
  <br>
</center>

`LPF`를 써도 결국 잘린 부분이 저렇게 비스듬하게 꺾이면서 약간 아쉬운 부분이 남게된다. 이때 그림에 표시된 허용 범위인 `-3db` 밑으로 떨어지기 시작하는 곳을 `Cut Off`라고 부른다. 저기에서부터 신호가 잘렸다고 치겠다는 것이다. 그럼 `Cut Off`되는 부분을 `20kHz`보다 조금 더 밑으로 내리면 `20kHz` 언저리에서 신호가 사라지게 만들 수 있지 않을까? 라는 생각도 들지만 그 문제 때문에  `20kHz`의 주파수 영역을 전부 활용하지 못하는 건 너무 아깝다고 판단이 들었나보다.

그래서 `Cut Off`를 딱 `20kHz`에 맞추고 남는 부분은 그냥 감수하자고 합의가 된 것이다. 이때 저 남는 부분이 딱 `2,050hz`다. 그렇다면 남는 부분과 가청주파수를 합치면 `22,050hz`가 된다. 나이퀴스트 이론에 따르면 우리는 최소한 2배의 샘플 레이트를 준비해야 이 신호를 제대로 측정할 수 있으므로 결국 CD의 표준 샘플 레이트가 `44,100hz = 44.1kHz`가 된 것이다.
뭐 그 외에도 당시 기술의 한계로 인해 용량 문제라던가 기업들끼리 싸우기도 하고 어른의 사정도 있는 등 국제 표준을 정할 때 늘 발생하는 여러가지 이슈가 있지만 대표적인 기술적인 이슈는 이 이유였다.

그 후 `48kHz`, `96kHz`, `192kHz` 등의 높은 샘플 레이트는 그냥 디바이스가 발전하면서 기술적인 제한이 없어졌으니까 `샘플 레이트는 클수록 좋지! 뿜뿜`하면서 늘린 것이다.

### 비트 레이트(Bit Rate)
`비트 레이트`는 샘플링에 비하면 초 간단하다. 특히 우리 같은 개발랭이들에게 익숙한 이름인 `Bit`가 붙어있지 않은가? 샘플 레이트가 소리의 가로 해상도 역할을 한다면 `비트 레이트`는 세로 해상도 역할을 한다.

샘플 레이트 때도 그랬듯이 CD를 기준으로 설명하는 게 보편적이기 때문에 다시 CD를 기준으로 설명하겠다.

CD의 비트 레이트는 `16bit`인데 이건 말 그대로 세로로 `16bit` 만큼의 해당하는 값을 표현할 수 있다는 얘기이다. 아까 위에서 설명한 샘플링을 진행할 때 전압을 측정했었다. 이때 컴퓨터가 측정한 이 전압의 값을 얼마나 섬세하게 표현할 수 있느냐를 비트 레이트가 결정하는 것이다. `16bit`면 16자리의 이진법을 사용할 수 있다는 것이고 $2^{16} = 65536$이니까 `0~65536`까지 총 `65537`개의 값을 사용할 수 있는 것이다.

<center>
  {% asset_img bitrate.png 500 %}
  <small>비트 레이트가 높을 수록 아날로그 신호 내부의 막대가 더 꼼꼼하게 채워지는 것을 볼 수 있다.</small>
  <small>+와 -를 합쳐서 비트를 세는 `signed`이기 때문에 그림에는 50% 씩만 표현되어있는 것이다.</small>
  <br>
</center>

물론 아날로그인 소리 신호가 변환된 전압 값이 `123` 같이 딱 떨어지는 정수일리가 없으므로 우리가 사용할 수 있는 `0~65536` 중 근사치를 찾아서 바꿔주는데 이 과정을 `양자화(Quatizing)`이라고 부른다. <small>(양자역학에서 나오는 그 양자랑 같은 의미 맞다.)</small>

이후 `0~65536`의 값으로 변경된 전압을 컴퓨터가 이해할 수 있는 `이진수(Binary)`로 변경하는 과정을 `부호화(Coding)`라고 한다.

이런 것들은 우리같은 개발랭이들은 워낙 익숙한 문제이기 때문에 이 정도만 설명하고 넘어가겠다.

## Audio Context API로 파형 그려보기
자, 드디어 길고 길었던 소리 이론이 끝났다. 필자는 디지털로 변환된 오디오를 가지고 파형을 그려보는 것이 목적이기 때문에 `Analog to Digital`만 다뤘고 `Digital to Analog`는 이 포스팅에서 다루지 않겠다. 근데 이것도 나름 재밌으므로 따로 찾아보길 강추한다.

필자는 오디오 파일을 업로드해서 해당 오디오 파일을 `Audio Context API`로 분석하여 필요한 데이터를 뽑아내고 `svg`를 사용하여 파형을 그릴 것이다.
참고로 프로젝트는 `webpack4`와 `babel7`을 사용하여 초간단하게 구성했다. 자세한 코드는 [깃허브 레파지토리](https://github.com/evan-moon/simple-waveform-visualizer)에서 확인 해볼 수 있다.


### 기본 틀 잡기
그럼 먼저 HTML을 간단하게 작성하자.

```html index.html
<body>
  <input id="audio-uploader" type="file">
  <svg id="waveform" preserveAspectRatio="none">
    <g id="waveform-path-group"></g>
  </svg>
</body>
```

HTML은 이게 끝이다. 오디오 파일을 업로드할 `input` 엘리먼트 하나와 파형을 그릴 `svg` 엘리먼트 하나만 있으면 된다. 어차피 테스트 용도라서 UI가 중요한게 아니기 때문에 기능 구현에 충실했다.<small>(라고 포장을 해봅니다)</small>
이제 파일이 업로드되면 실행될 이벤트 핸들러를 작성하자.

```js index.js
(function () {
  const inputDOM = document.getElementById('audio-uploader');
  inputDOM.onchange = e => {
    const file = e.currentTarget.files[0];
    if (file) {
      const reader = new FileReader();
      reader.onload = e => console.log(e.target.result);
      reader.readAsArrayBuffer(file);
    }
  }
})();
```

자, 여기까지 작성하고나서 필자가 좋아하는 노래인 `거미의 그대돌아오면` mp3 파일을 한번 업로드 해보았다. `FileReader.prototype.readAsArrayBuffer` 메소드는 바이너리 형태인 파일을 한번에 반환하는 것이 아니라 일정 단위의 `청크(Chunk)`로 반환하는 메소드이다. 원래는 이 `ArrayBuffer`를 처리해주기 위한 별도의 로직을 작성해야하지만 이번에 사용할 `Audio Context API`가 내부적으로 `ArrayBuffer`를 알아서 처리해주기 때문에 걱정하지 않아도 된다.

### AudioAnalyzer 클래스 작성
이제 본격적인 `Audio Context API`를 사용해볼 차례이다. 필자는 별도로 `AudioAnalyzer`라는 싱글톤 클래스를 만들었다.

```js lib/AudioAnalyzer.js
class AudioAnalyzer {
  constructor () {
    if (!window.AudioContext) {
      const errorMsg = 'Web Audio API 지원 안돼유 ㅜㅜ';
      alert(errorMsg);
      throw new Error(errorMsg);
    }

    this.audioContext = new (AudioContext || webkitAudioContext)();
    this.sourceBuffer = this.audioContext.createBufferSource();
    this.sampleRate = 0;
    this.peaks = [];

    this.waveFormBox = document.getElementById('waveform');
    this.waveFormPathGroup = document.getElementById('waveform-path-group');
  }

  reset () {
    this.audioContext = new (AudioContext || webkitAudioContext)();
    this.sourceBuffer = this.audioContext.createBufferSource();
    this.sampleRate = 0;
    this.peaks = [];
    this.updateViewboxSize();
  }
}

export default new AudioAnalyzer();
```

이렇게 대충 기본 틀을 잡아주고 이 클래스에 오디오 파일을 Set 할 수 있는 귀여운 setter를 하나 선언했다.

```js
setAudio (audioFile) {
  this.audioContext.decodeAudioData(audioFile).then(buffer => {
    this.sourceBuffer.buffer = buffer;
    this.sampleRate = buffer.sampleRate;
    this.updateViewboxSize();
    this.setPeaks();
    this.draw();
  });
}
```

`AudioContext.prototype.decodeAudioData` 메소드는 `ArrayBuffer`를 받아서 `AudioBuffer`로 변환하는 메소드이다. 아까 위에서 설명했던 대로 `readAsArrayBuffer` 메소드가 반환한 `ArrayBuffer`는 한번에 모든 바이너리 데이터를 불러오는 것이 아니라 청크 단위로 불러오기 때문에 `decodeAudioData`도 변환을 동기적으로 해주지는 않는다. 그래서 이 메소드를 사용할 때는 콜백함수를 사용하거나 `Promise`를 사용해야 한다.

필자는 콜백 혐오자이기 때문에 `Promise`를 사용했다.

이제 실제로 오디오 파일이 어떻게 변환되는지 보기 위해 파일을 업로드하면 `AudioAnalyzer`에게 오디오 파일을 넘기도록 변경해주자.

```js index.js
import AudioAnalyzer from './lib/AudioAnalyzer';

(function () {
  const inputDOM = document.getElementById('audio-uploader');
  inputDOM.onchange = e => {
    const file = e.currentTarget.files[0];
    if (file) {
      // AudioAnalyzer 초기화
      AudioAnalyzer.reset();
      const reader = new FileReader();
      // AudioAnalyzer에게 파일 토스
      reader.onload = e => AudioAnalyzer.setAudio(e.target.result);
      reader.readAsArrayBuffer(file);
    }
  }
})();
```

이 쯤 했으면 이제 메인 함수에는 더 이상 뭘 작성할 필요가 없을 것 같다. 이제 `AudioAnalyzer`만 가지고 놀면 된다. 한번 다시 `그대 돌아오면.mp3` 파일을 업로드 해보았다.

```js decodeAudioData
AudioBuffer {
  length: 12225071,
  duration: 277.2124943310658,
  sampleRate: 44100,
  numberOfChannels: 2
}
```

그러면 `AudioAnalyzer`의 `setAudio` 메소드 내부에서 실행된 `decodeAudioData`가 `ArrayBuffer`를 받아서 `AudioBuffer`로 변환한 뒤 반환해준다. 이걸 뜯어보면 유용한 정보들이 들어있다.

- `sampleRate`: 당연히 샘플 레이트를 의미하고 이 곡의 샘플 레이트는 `44,100hz`이다.
- `numberOfChannels`: 이 오디오가 몇개의 채널을 가지고 있는지를 나타내는데 이 파일은 두 개의 채널이 있는 스테레오 채널 오디오 파일이다.
- `length`: 샘플링된 `peak`들의 개수를 의미한다. 아까 샘플링 설명할 때 그림에 찍혀있던 점들이다.
- `duration`: 이 오디오 파일의 재생 길이를 `초`단위로 표시해준다. `그대 돌아오면`은 약 `277초`동안 재생되나보다.

### AudioAnalyzer 가지고 놀기

